
 <!DOCTYPE HTML>
<html >
<head>
  <meta charset="UTF-8">
  
    <title>《machine learning with spark》学习笔记--文本挖掘 | XingLiu&#39;s  Blog</title>
    <meta name="viewport" content="width=device-width, initial-scale=1,user-scalable=no">
    
    <meta name="author" content="XingLiu">
    

    
    <meta name="description" content="We will introduce more advanced text processing techniques available in MLlib to work with large-scale text datasets.">
<meta property="og:type" content="article">
<meta property="og:title" content="《machine learning with spark》学习笔记--文本挖掘">
<meta property="og:url" content="http://pangjiuzala.github.io/2016/02/15/《machine-learning-with-spark》学习笔记-文本挖掘/index.html">
<meta property="og:site_name" content="XingLiu's  Blog">
<meta property="og:description" content="We will introduce more advanced text processing techniques available in MLlib to work with large-scale text datasets.">
<meta property="og:updated_time" content="2016-02-15T13:15:23.000Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="《machine learning with spark》学习笔记--文本挖掘">
<meta name="twitter:description" content="We will introduce more advanced text processing techniques available in MLlib to work with large-scale text datasets.">

    
    <link rel="alternative" href="https://github.com/search?q=pangjiuzala&type=Users" title="XingLiu&#39;s  Blog" type="application/atom+xml">
    
    
    <link rel="icon" href="/img/favicon.ico">
    
    
    <link rel="apple-touch-icon" href="/img/jacman.jpg">
    <link rel="apple-touch-icon-precomposed" href="/img/jacman.jpg">
    
    <link rel="stylesheet" href="/css/style.css" type="text/css">
</head>

  <body>
    <header>
      
<div>
		
			<div id="imglogo">
				<a href="/"><img src="/img/logo.png" alt="XingLiu&#39;s  Blog" title="XingLiu&#39;s  Blog"/></a>
			</div>
			
			<div id="textlogo">
				<h1 class="site-name"><a href="/" title="XingLiu&#39;s  Blog">XingLiu&#39;s  Blog</a></h1>
				<h2 class="blog-motto"></h2>
			</div>
			<div class="navbar"><a class="navbutton navmobile" href="#" title="菜单">
			</a></div>
			<nav class="animated">
				<ul>
					<ul>
					 
						<li><a href="/">主页</a></li>
					
						<li><a href="/archives">文章列表</a></li>
					
					<li>
 					
					<form class="search" action="/search/index.html" method="get" accept-charset="utf-8">
						<label>Search</label>
						<input type="search" id="search" autocomplete="off" name="q" maxlength="20" placeholder="搜索" />
					</form>
					
					</li>
				</ul>
			</nav>			
</div>
    </header>
    <div id="container">
      <div id="main" class="post" itemscope itemprop="blogPost">
  
	<article itemprop="articleBody"> 
		<header class="article-info clearfix">
  <h1 itemprop="name">
    
      <a href="/2016/02/15/《machine-learning-with-spark》学习笔记-文本挖掘/" title="《machine learning with spark》学习笔记--文本挖掘" itemprop="url">《machine learning with spark》学习笔记--文本挖掘</a>
  </h1>
  <p class="article-author">By
       
		<a href="/about" title="XingLiu" target="_blank" itemprop="author">XingLiu</a>
		
  <p class="article-time">
    <time datetime="2016-02-15T13:14:18.000Z" itemprop="datePublished"> 发表于 2016-02-15</time>
    
  </p>
</header>
	<div class="article-content">
		
		<div id="toc" class="toc-article">
			<strong class="toc-title">文章目录</strong>
		
			
		
		</div>
		
		<p>We will introduce more advanced text processing techniques available in MLlib to work with large-scale text datasets.</p>
<a id="more"></a>
<p>In this article, we will:</p>
<ul>
<li>Work through detailed examples that illustrate data processing, feature<br>extraction, and the modeling pipeline, as they relate to text data</li>
<li>Evaluate the similarity between two documents based on the words in<br>the documents</li>
<li>Use the extracted text features as inputs for a classification model</li>
<li>Cover a recent development in natural language processing to model words themselves as vectors and illustrate the use of Spark’s Word2Vec model to evaluate the similarity between two words, based on their meaning</li>
</ul>
<p><strong>DataSource</strong></p>
<p><a href="http://qwone.com/~jason/20Newsgroups/" target="_blank" rel="external"><strong>20news-bydate.tar.gz </strong></a></p>
<p><strong>SourceCode</strong></p>
<figure class="highlight http"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="kotlin"><span class="keyword">import</span> org.apache.spark.SparkConf</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.SparkContext</span><br><span class="line"><span class="keyword">import</span> com.sun.org.apache.xalan.internal.xsltc.compiler.Whitespace</span><br><span class="line"><span class="keyword">import</span> scala.util.matching.Regex</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.ml.feature.StopWords</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.mllib.feature.HashingTF</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.mllib.linalg.&#123; SparseVector =&gt; SV &#125;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.mllib.feature.IDF</span><br><span class="line"><span class="keyword">import</span> breeze.linalg.SparseVector</span><br><span class="line"><span class="keyword">import</span> breeze.linalg.norm</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.mllib.regression.LabeledPoint</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.mllib.classification.NaiveBayes</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.mllib.evaluation.MulticlassMetrics</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.mllib.feature.Word2Vec</span><br><span class="line"><span class="keyword">object</span> AdvanceText &#123;</span><br><span class="line">  def tokenize(line: String, regex: Regex, stopwords: String, rareTokens: String): Seq[String] = &#123;</span><br><span class="line">    line.split(<span class="string">""</span><span class="string">"\W+"</span><span class="string">""</span>)</span><br><span class="line">      .map(_.toLowerCase)</span><br><span class="line">      .filter(token =&gt; regex.pattern.matcher(token).matches)</span><br><span class="line">      .filterNot(token =&gt; stopwords.contains(token))</span><br><span class="line">      .filterNot(token =&gt; rareTokens.contains(token))</span><br><span class="line">      .filter(token =&gt; token.size &gt;= <span class="number">2</span>)</span><br><span class="line">      .toSeq</span><br><span class="line">  &#125;</span><br><span class="line">  def main(args: Array[String]): <span class="typename">Unit</span> = &#123;</span><br><span class="line">    <span class="variable"><span class="keyword">val</span> path</span> = <span class="string">"hdfs://master:9000/user/root/input/20news-bydate-train/*"</span></span><br><span class="line">    <span class="variable"><span class="keyword">val</span> conf</span> = new SparkConf().setAppName(<span class="string">"text"</span>)</span><br><span class="line">    <span class="variable"><span class="keyword">val</span> sc</span> = new SparkContext(conf)</span><br><span class="line">    <span class="variable"><span class="keyword">val</span> rdd</span> = sc.wholeTextFiles(path)</span><br><span class="line">    <span class="comment">//    val text = rdd.map &#123;</span></span><br><span class="line">    <span class="comment">//      case (file, text) =&gt; text</span></span><br><span class="line">    <span class="comment">//    &#125;</span></span><br><span class="line">    <span class="comment">// println(text.count)</span></span><br><span class="line">    <span class="variable"><span class="keyword">val</span> newgroups</span> = rdd.map &#123;</span><br><span class="line">      case (file, text) =&gt;</span><br><span class="line">        file.split(<span class="string">"/"</span>).takeRight(<span class="number">2</span>).head</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="variable"><span class="keyword">val</span> countByGroup</span> = newgroups.map(n =&gt; (n, <span class="number">1</span>)).reduceByKey(_ + _).collect.sortBy(-_._2).mkString(<span class="string">"\n"</span>)</span><br><span class="line">    <span class="comment">//println(countByGroup)</span></span><br><span class="line">    <span class="variable"><span class="keyword">val</span> text</span> = rdd.map &#123;</span><br><span class="line">      case (file, text) =&gt; text</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="variable"><span class="keyword">val</span> whileSpaceSplit</span> = text.flatMap(t =&gt; t.split(<span class="string">" "</span>).</span><br><span class="line">      map(_.toLowerCase))</span><br><span class="line">    <span class="comment">//println(whileSpaceSplit.distinct.count)</span></span><br><span class="line">    <span class="comment">// println(whileSpaceSplit.sample(true, 0.3, 42).take(100).mkString(","))</span></span><br><span class="line">    <span class="variable"><span class="keyword">val</span> nonWordSplit</span> = text.flatMap(t =&gt; t.split(<span class="string">""</span><span class="string">"\W+"</span><span class="string">""</span>).map(_.toLowerCase))</span><br><span class="line">    <span class="comment">// println(nonWordSplit.distinct.count)</span></span><br><span class="line">    <span class="comment">//println(nonWordSplit.distinct.sample(true, 0.3, 42).take(100).mkString(","))</span></span><br><span class="line">    <span class="variable"><span class="keyword">val</span> regex</span> = <span class="string">""</span><span class="string">"[^0-9]*"</span><span class="string">""</span>.r</span><br><span class="line">    <span class="variable"><span class="keyword">val</span> filterNumbers</span> = nonWordSplit.filter(token =&gt;</span><br><span class="line">      regex.pattern.matcher(token).matches)</span><br><span class="line">    <span class="comment">// println(filterNumbers.distinct.count)</span></span><br><span class="line">    <span class="comment">//println(filterNumbers.distinct.sample(true, 0.3, 42).take(100).mkString(","))</span></span><br><span class="line">    <span class="variable"><span class="keyword">val</span> tokenCounts</span> = filterNumbers.map(t =&gt; (t, <span class="number">1</span>)).reduceByKey(_ + _)</span><br><span class="line">    <span class="variable"><span class="keyword">val</span> oreringDesc</span> = Ordering.by[(String, <span class="typename">Int</span>), <span class="typename">Int</span>](_._2)</span><br><span class="line">    <span class="comment">//println(tokenCounts.top(20)(oreringDesc).mkString("\n"))</span></span><br><span class="line">    <span class="variable"><span class="keyword">val</span> stopwords</span> = Set(</span><br><span class="line">      <span class="string">"the"</span>, <span class="string">"a"</span>, <span class="string">"an"</span>, <span class="string">"of"</span>, <span class="string">"or"</span>, <span class="string">"in"</span>, <span class="string">"for"</span>, <span class="string">"by"</span>, <span class="string">"on"</span>, <span class="string">"but"</span>, <span class="string">"is"</span>, <span class="string">"not"</span>,</span><br><span class="line">      <span class="string">"with"</span>, <span class="string">"as"</span>, <span class="string">"was"</span>, <span class="string">"if"</span>,</span><br><span class="line">      <span class="string">"they"</span>, <span class="string">"are"</span>, <span class="string">"this"</span>, <span class="string">"and"</span>, <span class="string">"it"</span>, <span class="string">"have"</span>, <span class="string">"from"</span>, <span class="string">"at"</span>, <span class="string">"my"</span>,</span><br><span class="line">      <span class="string">"be"</span>, <span class="string">"that"</span>, <span class="string">"to"</span>)</span><br><span class="line">    <span class="variable"><span class="keyword">val</span> tokenCountsFilteredStopwords</span> = tokenCounts.filter &#123;</span><br><span class="line">      case (k, v) =&gt;</span><br><span class="line">        !stopwords.contains(k)</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="comment">//println(tokenCountsFilteredStopwords.top(20)(oreringDesc).mkString("\n"))</span></span><br><span class="line">    <span class="variable"><span class="keyword">val</span> tokenCountsFiltereredSize</span> = tokenCountsFilteredStopwords.filter &#123;</span><br><span class="line">      case (k, v) =&gt; k.size &gt;= <span class="number">2</span></span><br><span class="line"></span><br><span class="line">    &#125;</span><br><span class="line">    <span class="comment">//println(tokenCountsFiltereredSize.top(20)(oreringDesc).mkString("\n"))</span></span><br><span class="line">    <span class="variable"><span class="keyword">val</span> rareTokens</span> = tokenCounts.filter &#123; case (k, v) =&gt; v <span class="type">&lt; 2 &#125;.map &#123;</span><br><span class="line">      case (k, v) =&gt;</span> k</span><br><span class="line">    &#125;.collect.toSet</span><br><span class="line">    <span class="variable"><span class="keyword">val</span> tokenCountsFilteredAll</span> = tokenCountsFiltereredSize.filter &#123;</span><br><span class="line">      case (k, v) =&gt; !rareTokens.contains(k)</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="comment">//println(tokenCountsFilteredAll.top(20)(oreringDesc).mkString("\n"))</span></span><br><span class="line">    <span class="variable"><span class="keyword">val</span> tokens</span> = text.map(doc =&gt; tokenize(doc, regex, stopwords.toString(), rareTokens.toString()))</span><br><span class="line">    <span class="variable"><span class="keyword">val</span> dim</span> = math.pow(<span class="number">2</span>, <span class="number">18</span>).toInt</span><br><span class="line">    <span class="variable"><span class="keyword">val</span> hashingTF</span> = new HashingTF(dim)</span><br><span class="line">    <span class="variable"><span class="keyword">val</span> tf</span> = hashingTF.transform(tokens)</span><br><span class="line">    tf.cache</span><br><span class="line">    <span class="comment">/*conculate tf*/</span></span><br><span class="line">    <span class="variable"><span class="keyword">val</span> v</span> = tf.first.asInstanceOf[SV]</span><br><span class="line">    <span class="comment">//println(v.size)</span></span><br><span class="line">    <span class="comment">//println(v.values.size)</span></span><br><span class="line">    <span class="comment">//println(v.values.take(10).toSeq)</span></span><br><span class="line">    <span class="comment">//println(v.indices.take(10).toSeq)</span></span><br><span class="line">    <span class="comment">/*conculate idf*/</span></span><br><span class="line">    <span class="variable"><span class="keyword">val</span> idf</span> = new IDF().fit(tf)</span><br><span class="line">    <span class="variable"><span class="keyword">val</span> tfidf</span> = idf.transform(tf)</span><br><span class="line">    <span class="variable"><span class="keyword">val</span> v2</span> = tfidf.first.asInstanceOf[SV]</span><br><span class="line">    <span class="comment">//println(v2.size)</span></span><br><span class="line">    <span class="comment">//println(v2.values.size)</span></span><br><span class="line">    <span class="comment">//println(v2.values.take(10).toSeq)</span></span><br><span class="line">    <span class="comment">//println(v2.indices.take(10).toSeq)</span></span><br><span class="line">    <span class="variable"><span class="keyword">val</span> minMaxVals</span> = tfidf.map &#123; v =&gt;</span><br><span class="line">      <span class="variable"><span class="keyword">val</span> sv</span> = v.asInstanceOf[SV]</span><br><span class="line">      (sv.values.min, sv.values.max)</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="variable"><span class="keyword">val</span> globalMinMax</span> = minMaxVals.reduce &#123;</span><br><span class="line">      case ((min1, max1), (min2, max2)) =&gt;</span><br><span class="line">        (math.min(min1, min2), math.max(max1, max2))</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="comment">//println(globalMinMax)</span></span><br><span class="line">    <span class="variable"><span class="keyword">val</span> common</span> = sc.parallelize(Seq(Seq(<span class="string">"you"</span>, <span class="string">"do"</span>, <span class="string">"we"</span>)))</span><br><span class="line">    <span class="variable"><span class="keyword">val</span> tfCommon</span> = hashingTF.transform(common)</span><br><span class="line">    <span class="variable"><span class="keyword">val</span> tfidfCommon</span> = idf.transform(tfCommon)</span><br><span class="line">    <span class="variable"><span class="keyword">val</span> commonVector</span> = tfidfCommon.first.asInstanceOf[SV]</span><br><span class="line">    <span class="comment">//println(commonVector.values.toSeq)</span></span><br><span class="line">    <span class="variable"><span class="keyword">val</span> uncommon</span> = sc.parallelize(Seq(Seq(<span class="string">"telescope"</span>, <span class="string">"legislation"</span>,</span><br><span class="line">      <span class="string">"investment"</span>)))</span><br><span class="line">    <span class="variable"><span class="keyword">val</span> tfUncommon</span> = hashingTF.transform(uncommon)</span><br><span class="line">    <span class="variable"><span class="keyword">val</span> tfidfUncommon</span> = idf.transform(tfUncommon)</span><br><span class="line">    <span class="variable"><span class="keyword">val</span> uncommonVector</span> = tfidfUncommon.first.asInstanceOf[SV]</span><br><span class="line">    <span class="comment">//println(uncommonVector.values.toSeq)</span></span><br><span class="line">    <span class="variable"><span class="keyword">val</span> hockeyText</span> = rdd.filter &#123;</span><br><span class="line">      case (file, text) =&gt; file.contains(<span class="string">"honey"</span>)</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="variable"><span class="keyword">val</span> hockeyTF</span> = hockeyText.mapValues(doc =&gt;</span><br><span class="line">      hashingTF.transform(tokenize(doc, regex, stopwords.toString(), rareTokens.toString())))</span><br><span class="line">    <span class="variable"><span class="keyword">val</span> hockeyTFIdf</span> = idf.transform(hockeyTF.map(_._2))</span><br><span class="line">    <span class="variable"><span class="keyword">val</span> hockey1</span> = hockeyTFIdf.sample(<span class="literal">true</span>, <span class="number">0.1</span>, <span class="number">42</span>).first.asInstanceOf[SV]</span><br><span class="line">    <span class="variable"><span class="keyword">val</span> breeze1</span> = new SparseVector(hockey1.indices, hockey1.values, hockey1.size)</span><br><span class="line">    <span class="variable"><span class="keyword">val</span> hockey2</span> = hockeyTFIdf.sample(<span class="literal">true</span>, <span class="number">0.1</span>, <span class="number">43</span>).first.asInstanceOf[SV]</span><br><span class="line">    <span class="variable"><span class="keyword">val</span> breeze2</span> = new SparseVector(hockey2.indices, hockey2.values, hockey2.size)</span><br><span class="line">    <span class="variable"><span class="keyword">val</span> cosineSim</span> = breeze1.dot(breeze2) / (norm(breeze1) * norm(breeze2))</span><br><span class="line">    <span class="comment">//println(cosineSim)</span></span><br><span class="line">    <span class="variable"><span class="keyword">val</span> graphicsText</span> = rdd.filter &#123;</span><br><span class="line">      case (file, text) =&gt;</span><br><span class="line">        file.contains(<span class="string">"comp.graphics"</span>)</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="variable"><span class="keyword">val</span> graphicsTF</span> = graphicsText.mapValues(doc =&gt;</span><br><span class="line">      hashingTF.transform(tokenize(doc, regex, stopwords.toString(), rareTokens.toString())))</span><br><span class="line">    <span class="variable"><span class="keyword">val</span> graphicsTFIdf</span> = idf.transform(graphicsTF.map(_._2))</span><br><span class="line">    <span class="variable"><span class="keyword">val</span> graphics</span> = graphicsTFIdf.sample(<span class="literal">true</span>, <span class="number">0.1</span>, <span class="number">42</span>).first.asInstanceOf[SV]</span><br><span class="line">    <span class="variable"><span class="keyword">val</span> breezeGraphics</span> = new SparseVector(graphics.indices, graphics.values, graphics.size)</span><br><span class="line">    <span class="variable"><span class="keyword">val</span> cosineSim2</span> = breeze1.dot(breezeGraphics) / (norm(breeze1) * norm(breezeGraphics))</span><br><span class="line">    println(cosineSim2)</span><br><span class="line">    <span class="variable"><span class="keyword">val</span> baseballText</span> = rdd.filter &#123;</span><br><span class="line">      case (file, text) =&gt;</span><br><span class="line">        file.contains(<span class="string">"baseball"</span>)</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="variable"><span class="keyword">val</span> baseballTF</span> = baseballText.mapValues(doc =&gt;</span><br><span class="line">      hashingTF.transform(tokenize(doc, regex, stopwords.toString(), rareTokens.toString())))</span><br><span class="line">    <span class="variable"><span class="keyword">val</span> baseballTfIdf</span> = idf.transform(baseballTF.map(_._2))</span><br><span class="line">    <span class="variable"><span class="keyword">val</span> baseball</span> = baseballTfIdf.sample(<span class="literal">true</span>, <span class="number">0.1</span>, <span class="number">42</span>).first.asInstanceOf[SV]</span><br><span class="line">    <span class="variable"><span class="keyword">val</span> breezeBaseball</span> = new SparseVector(baseball.indices,</span><br><span class="line">      baseball.values, baseball.size)</span><br><span class="line">    <span class="variable"><span class="keyword">val</span> consineSim3</span> = breeze1.dot(breezeBaseball) / (norm(breeze1) *</span><br><span class="line">      norm(breezeBaseball))</span><br><span class="line">    println(consineSim3)</span><br><span class="line">    <span class="variable"><span class="keyword">val</span> newsgroupsMap</span> = newgroups.distinct.collect().zipWithIndex.toMap</span><br><span class="line">    <span class="variable"><span class="keyword">val</span> zipped</span> = newgroups.zip(tfidf)</span><br><span class="line">    <span class="variable"><span class="keyword">val</span> train</span> = zipped.map &#123; case (topic, vector) =&gt; LabeledPoint(newsgroupsMap(topic), vector) &#125;</span><br><span class="line">    train.cache</span><br><span class="line">    <span class="variable"><span class="keyword">val</span> model</span> = NaiveBayes.train(train, lambda = <span class="number">0.1</span>)</span><br><span class="line">    <span class="variable"><span class="keyword">val</span> testPath</span> = <span class="string">"hdfs://master:9000/user/root/input/20news-bydate-test/*"</span></span><br><span class="line">    <span class="variable"><span class="keyword">val</span> testRDD</span> = sc.wholeTextFiles(testPath)</span><br><span class="line">    <span class="variable"><span class="keyword">val</span> testLabels</span> = testRDD.map &#123;</span><br><span class="line">      case (file, text) =&gt;</span><br><span class="line">        <span class="variable"><span class="keyword">val</span> topic</span> = file.split(<span class="string">"/"</span>).takeRight(<span class="number">2</span>).head</span><br><span class="line">        newsgroupsMap(topic)</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="variable"><span class="keyword">val</span> testTf</span> = testRDD.map &#123;</span><br><span class="line">      case (file, text) =&gt;</span><br><span class="line">        hashingTF.transform(tokenize(text, regex, stopwords.toString(), rareTokens.toString()))</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="variable"><span class="keyword">val</span> testTfIdf</span> = idf.transform(testTf)</span><br><span class="line">    <span class="variable"><span class="keyword">val</span> zippedTest</span> = testLabels.zip(testTfIdf)</span><br><span class="line">    <span class="variable"><span class="keyword">val</span> test</span> = zippedTest.map &#123;</span><br><span class="line">      case (topic, vector) =&gt;</span><br><span class="line">        LabeledPoint(topic, vector)</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="variable"><span class="keyword">val</span> predictionAndLabel</span> = test.map(p =&gt; (model.predict(p.features),</span><br><span class="line">      p.label))</span><br><span class="line">    <span class="variable"><span class="keyword">val</span> accuracy</span> = <span class="number">1.0</span> * predictionAndLabel.filter(x =&gt; x._1 == x._2).count() / test.count()</span><br><span class="line">    <span class="variable"><span class="keyword">val</span> metrics</span> = new MulticlassMetrics(predictionAndLabel)</span><br><span class="line">    println(accuracy)</span><br><span class="line">    println(metrics.weightedFMeasure)</span><br><span class="line">    <span class="variable"><span class="keyword">val</span> rawTokens</span> = rdd.map &#123; case (file, text) =&gt; text.split(<span class="string">" "</span>) &#125;</span><br><span class="line">    <span class="comment">//evaluate the performance on the test set as we did for the model trained with TF-IDF features:</span></span><br><span class="line">    <span class="variable"><span class="keyword">val</span> rawTF</span> = tokens.map(doc =&gt; hashingTF.transform(doc))</span><br><span class="line">    <span class="variable"><span class="keyword">val</span> rawTrain</span> = newgroups.zip(rawTF).map &#123;</span><br><span class="line">      case (topic, vector) =&gt;</span><br><span class="line">        LabeledPoint(newsgroupsMap(topic), vector)</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="variable"><span class="keyword">val</span> rawModel</span> = NaiveBayes.train(rawTrain, lambda = <span class="number">0.1</span>)</span><br><span class="line">    <span class="variable"><span class="keyword">val</span> rawTestTF</span> = testRDD.map &#123;</span><br><span class="line">      case (file, text) =&gt;</span><br><span class="line">        hashingTF.transform(text.split(<span class="string">" "</span>))</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="variable"><span class="keyword">val</span> rawZippedTest</span> = testLabels.zip(rawTestTF)</span><br><span class="line">    <span class="variable"><span class="keyword">val</span> rawTest</span> = rawZippedTest.map &#123;</span><br><span class="line">      case (topic, vector) =&gt;</span><br><span class="line">        LabeledPoint(topic, vector)</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="variable"><span class="keyword">val</span> rawPredictionAndLabel</span> = rawTest.map(p =&gt;</span><br><span class="line">      (rawModel.predict(p.features), p.label))</span><br><span class="line">    <span class="variable"><span class="keyword">val</span> rawAccuracy</span> = <span class="number">1.0</span> * rawPredictionAndLabel.filter(x =&gt; x._1 ==</span><br><span class="line">      x._2).count() / rawTest.count()</span><br><span class="line">    println(rawAccuracy)</span><br><span class="line">    <span class="variable"><span class="keyword">val</span> rawMetrics</span> = new MulticlassMetrics(rawPredictionAndLabel)</span><br><span class="line">    println(rawMetrics.weightedFMeasure)</span><br><span class="line">    <span class="variable"><span class="keyword">val</span> word2vec</span> = new Word2Vec()</span><br><span class="line">    word2vec.setSeed(<span class="number">42</span>)</span><br><span class="line">    <span class="variable"><span class="keyword">val</span> word2vecModel</span> = word2vec.fit(tokens)</span><br><span class="line">    word2vecModel.findSynonyms(<span class="string">"hockey"</span>, <span class="number">20</span>).foreach(println)</span><br><span class="line">    word2vecModel.findSynonyms(<span class="string">"legislation"</span>, <span class="number">20</span>).foreach(println)</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">&#125;</span></span><br></pre></td></tr></table></figure>
  
	</div>
		<footer class="article-footer clearfix">
<div class="article-catetags">


  <div class="article-tags">
  
  <span></span> <a href="/tags/Spark/">Spark</a><a href="/tags/机器学习/">机器学习</a>
  </div>

</div>



	<div class="article-share" id="share">
	
	<div class="share-jiathis">
	  
<div class="jiathis_style_24x24">
	<a class="jiathis_button_tsina"></a>
	<a class="jiathis_button_weixin"></a>
	<a class="jiathis_button_renren"></a>
	<a class="jiathis_button_qzone"></a>
	<a class="jiathis_button_googleplus"></a>
	<a class="jiathis_button_douban"></a>
	<a href="http://www.jiathis.com/share" class="jiathis jiathis_txt jtico jtico_jiathis" target="_blank"></a>
	<a class="jiathis_counter_style"></a>
</div>
<script type="text/javascript" >
    var jiathis_config={
    data_track_clickback:true,
    sm:"copy,renren,cqq",
    pic:"",
    summary:"",
    
  </script> 
<script type="text/javascript" src="//v3.jiathis.com/code/jia.js?uid=
" charset="utf-8"></script>      

	 </div>
	
	</div>


</footer>

   	       
	</article>
	
<nav class="article-nav clearfix">
 

<div class="next">
<a href="/2016/02/14/《machine-learning-with-spark》学习笔记-聚类/"  title="《machine learning with spark》学习笔记--聚类">
 <strong>下一篇：</strong><br/> 
 <span>《machine learning with spark》学习笔记--聚类
</span>
</a>
</div>

</nav>

	
<section id="comments" class="comment">
	<div class="ds-thread" data-thread-key="2016/02/15/《machine-learning-with-spark》学习笔记-文本挖掘/" data-title="《machine learning with spark》学习笔记--文本挖掘" data-url="http://pangjiuzala.github.io/2016/02/15/《machine-learning-with-spark》学习笔记-文本挖掘/"></div>
</section>


</div>  
      <div class="openaside"><a class="navbutton" href="#" title="显示侧边栏"></a></div>

  <div id="toc" class="toc-aside">
  <strong class="toc-title">文章目录</strong>
 
 
 
  </div>

<div id="asidepart">
<div class="closeaside"><a class="closebutton" href="#" title="隐藏侧边栏"></a></div>
<aside class="clearfix">

  <div class="weiboshow">
  <p class="asidetitle">新浪微博</p>
    <iframe width="100%" height="119" class="share_self"  frameborder="0" scrolling="no" src="http://widget.weibo.com/weiboshow/index.php?language=&width=0&height=119&fansRow=2&ptype=1&speed=0&skin=9&isTitle=1&noborder=1&isWeibo=0&isFans=0&uid=2129798793&verifier=c0951e84&dpc=1"></iframe>
</div>


  
<div class="tagslist">
	<p class="asidetitle">标签</p>
		<ul class="clearfix">
		
			
				<li><a href="/tags/Java/" title="Java">Java<sup>31</sup></a></li>
			
		
			
				<li><a href="/tags/大数据/" title="大数据">大数据<sup>26</sup></a></li>
			
		
			
				<li><a href="/tags/性能优化/" title="性能优化">性能优化<sup>14</sup></a></li>
			
		
			
				<li><a href="/tags/数据挖掘/" title="数据挖掘">数据挖掘<sup>13</sup></a></li>
			
		
			
				<li><a href="/tags/HBase/" title="HBase">HBase<sup>12</sup></a></li>
			
		
			
				<li><a href="/tags/Hadoop/" title="Hadoop">Hadoop<sup>10</sup></a></li>
			
		
			
				<li><a href="/tags/算法/" title="算法">算法<sup>10</sup></a></li>
			
		
			
				<li><a href="/tags/Spark/" title="Spark">Spark<sup>6</sup></a></li>
			
		
			
				<li><a href="/tags/机器学习/" title="机器学习">机器学习<sup>6</sup></a></li>
			
		
			
				<li><a href="/tags/数据库/" title="数据库">数据库<sup>5</sup></a></li>
			
		
			
				<li><a href="/tags/网络爬虫/" title="网络爬虫">网络爬虫<sup>3</sup></a></li>
			
		
			
				<li><a href="/tags/移动互联网/" title="移动互联网">移动互联网<sup>3</sup></a></li>
			
		
			
				<li><a href="/tags/物联网/" title="物联网">物联网<sup>2</sup></a></li>
			
		
			
				<li><a href="/tags/Android/" title="Android">Android<sup>2</sup></a></li>
			
		
			
				<li><a href="/tags/云平台/" title="云平台">云平台<sup>2</sup></a></li>
			
		
			
				<li><a href="/tags/Taobao-File-System/" title="Taobao File System">Taobao File System<sup>1</sup></a></li>
			
		
			
				<li><a href="/tags/智慧医疗/" title="智慧医疗">智慧医疗<sup>1</sup></a></li>
			
		
			
				<li><a href="/tags/设计模式/" title="设计模式">设计模式<sup>1</sup></a></li>
			
		
			
				<li><a href="/tags/爬虫/" title="爬虫">爬虫<sup>1</sup></a></li>
			
		
			
				<li><a href="/tags/云计算/" title="云计算">云计算<sup>1</sup></a></li>
			
		
		</ul>
</div>


  
  <div class="archiveslist">
    <p class="asidetitle"><a href="/archives">归档</a></p>
      <ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2016/02/">February 2016</a><span class="archive-list-count">4</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2016/01/">January 2016</a><span class="archive-list-count">2</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2015/12/">December 2015</a><span class="archive-list-count">8</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2015/11/">November 2015</a><span class="archive-list-count">5</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2015/10/">October 2015</a><span class="archive-list-count">4</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2015/09/">September 2015</a><span class="archive-list-count">17</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2015/08/">August 2015</a><span class="archive-list-count">31</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2015/07/">July 2015</a><span class="archive-list-count">25</span></li></ul>
  </div>


  <div class="linkslist">
  <p class="asidetitle">友情链接</p>
    <ul>
        
          <li>
            
            	<a href="http://blog.csdn.net/pangjiuzala" target="_blank" title="My Blog in  CSDN">CSDN</a>
            
          </li>
        
          <li>
            
            	<a href="http://saliormoon.github.io/" target="_blank" title="王宇峰的博客">王宇峰的博客</a>
            
          </li>
        
          <li>
            
            	<a href="http://www.changhuiyuan.com/" target="_blank" title="常惠源的博客">常惠源的博客</a>
            
          </li>
        
    </ul>
</div>

  <div class="rsspart">
	<a href="https://github.com/search?q=pangjiuzala&amp;type=Users" target="_blank" title="关注刘兴的github">关注</a>
</div>

  

  


</aside>
</div>
    </div>
    <footer><div id="footer" >
	
	<div class="line">
		<span></span>
		<div class="author"></div>
	</div>
	
	
	<section class="info">
		<p> Hello,I&#39;m from ZjuCs! <br/>
			The more you diligent, the more you lucky!</p>
	</section>
	 
	<div class="social-font" class="clearfix">
		
		
		
		
		
		
		
		
		
		
	</div>
			
		

		<p class="copyright">
		Copyright@ 2015 Liuxing All rights reserved.
		</p>
</div>
</footer>
    <script src="/js/jquery-2.0.3.min.js"></script>
<script src="/js/jquery.imagesloaded.min.js"></script>
<script src="/js/gallery.js"></script>
<script src="/js/jquery.qrcode-0.12.0.min.js"></script>

<script type="text/javascript">
$(document).ready(function(){ 
  $('.navbar').click(function(){
    $('header nav').toggleClass('shownav');
  });
  var myWidth = 0;
  function getSize(){
    if( typeof( window.innerWidth ) == 'number' ) {
      myWidth = window.innerWidth;
    } else if( document.documentElement && document.documentElement.clientWidth) {
      myWidth = document.documentElement.clientWidth;
    };
  };
  var m = $('#main'),
      a = $('#asidepart'),
      c = $('.closeaside'),
      o = $('.openaside');
  c.click(function(){
    a.addClass('fadeOut').css('display', 'none');
    o.css('display', 'block').addClass('fadeIn');
    m.addClass('moveMain');
  });
  o.click(function(){
    o.css('display', 'none').removeClass('beforeFadeIn');
    a.css('display', 'block').removeClass('fadeOut').addClass('fadeIn');      
    m.removeClass('moveMain');
  });
  $(window).scroll(function(){
    o.css("top",Math.max(80,260-$(this).scrollTop()));
  });
  
  $(window).resize(function(){
    getSize(); 
    if (myWidth >= 1024) {
      $('header nav').removeClass('shownav');
    }else{
      m.removeClass('moveMain');
      a.css('display', 'block').removeClass('fadeOut');
      o.css('display', 'none');
      
      $('#toc.toc-aside').css('display', 'none');
        
    }
  });
});
</script>

<script type="text/javascript">
$(document).ready(function(){ 
  var ai = $('.article-content>iframe'),
      ae = $('.article-content>embed'),
      t  = $('#toc'),
      ta = $('#toc.toc-aside'),
      o  = $('.openaside'),
      c  = $('.closeaside');
  if(ai.length>0){
    ai.wrap('<div class="video-container" />');
  };
  if(ae.length>0){
   ae.wrap('<div class="video-container" />');
  };
  c.click(function(){
    ta.css('display', 'block').addClass('fadeIn');
  });
  o.click(function(){
    ta.css('display', 'none');
  });
  $(window).scroll(function(){
    ta.css("top",Math.max(140,320-$(this).scrollTop()));
  });
});
</script>




<script type="text/javascript">
  var duoshuoQuery = {short_name:"pangjiuzala"};
  (function() {
    var ds = document.createElement('script');
    ds.type = 'text/javascript';ds.async = true;
    ds.src = '//static.duoshuo.com/embed.js';
    ds.charset = 'UTF-8';
    (document.getElementsByTagName('head')[0] 
    || document.getElementsByTagName('body')[0]).appendChild(ds);
  })();
</script> 







<link rel="stylesheet" href="/fancybox/jquery.fancybox.css" media="screen" type="text/css">
<script src="/fancybox/jquery.fancybox.pack.js"></script>
<script type="text/javascript">
$(document).ready(function(){ 
  $('.article-content').each(function(i){
    $(this).find('img').each(function(){
      if ($(this).parent().hasClass('fancybox')) return;
      var alt = this.alt;
      if (alt) $(this).after('<span class="caption">' + alt + '</span>');
      $(this).wrap('<a href="' + this.src + '" title="' + alt + '" class="fancybox"></a>');
    });
    $(this).find('.fancybox').each(function(){
      $(this).attr('rel', 'article' + i);
    });
  });
  if($.fancybox){
    $('.fancybox').fancybox();
  }
}); 
</script>



<!-- Analytics Begin -->

<script type="text/javascript">
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');
ga('create', 'null', 'null');  
ga('send', 'pageview');
</script>



<script type="text/javascript">
var _bdhmProtocol = (("https:" == document.location.protocol) ? " https://" : " http://");
document.write(unescape("%3Cscript src='" + _bdhmProtocol + "hm.baidu.com/h.js%3Ffeafc504b70a541dd3845d467335f367' type='text/javascript'%3E%3C/script%3E"));
</script>



<script type="text/javascript">var cnzz_protocol = (("https:" == document.location.protocol) ? " https://" : " http://");document.write(unescape("%3Cspan id='cnzz_stat_icon_null'%3E%3C/span%3E%3Cscript src='" + cnzz_protocol + "s23.cnzz.com/z_stat.php%3Fid%3Dnull' type='text/javascript'%3E%3C/script%3E"));</script>

<!-- Analytics End -->

<!-- Totop Begin -->

	<div id="totop">
	<a title="返回顶部"><img src="/img/scrollup.png"/></a>
	</div>
	<script src="/js/totop.js"></script>

<!-- Totop End -->

<!-- MathJax Begin -->
<!-- mathjax config similar to math.stackexchange -->

<script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    tex2jax: {
      inlineMath: [ ['$','$'], ["\\(","\\)"] ],
      processEscapes: true
    }
  });
</script>

<script type="text/x-mathjax-config">
    MathJax.Hub.Config({
      tex2jax: {
        skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
      }
    });
</script>

<script type="text/x-mathjax-config">
    MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax(), i;
        for(i=0; i < all.length; i += 1) {
            all[i].SourceElement().parentNode.className += ' has-jax';
        }
    });
</script>

<script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script>


<!-- MathJax End -->

<!-- Tiny_search Begin -->

<script>
var option = {
  engineKey: '4ac092ad8d749fdc6293'
};
(function(w,d,t,u,n,s,e){
  s = d.createElement(t);
  s.src = u;
  s.async = 1;
  w[n] = function(r){
    w[n].opts = r;
  };
  e = d.getElementsByTagName(t)[0];
  e.parentNode.insertBefore(s, e);
})(window,document,'script','//tinysou-cdn.b0.upaiyun.com/ts.js','_ts');
_ts(option);
</script>

<!-- Tiny_search End -->

  </body>
</html>
