
 <!DOCTYPE HTML>
<html >
<head>
  <meta charset="UTF-8">
  
    <title>分类算法 | XingLiu&#39;s  Blog</title>
    <meta name="viewport" content="width=device-width, initial-scale=1,user-scalable=no">
    
    <meta name="author" content="XingLiu">
    

    
    <meta name="description" content="本文总结了数据挖掘以及机器学习中常见算法如神经网络算法、随机森林算法以及决策树等，希望能对数据挖掘爱好者有一定帮助。">
<meta property="og:type" content="article">
<meta property="og:title" content="分类算法">
<meta property="og:url" content="http://pangjiuzala.github.io/2015/07/31/分类算法/index.html">
<meta property="og:site_name" content="XingLiu's  Blog">
<meta property="og:description" content="本文总结了数据挖掘以及机器学习中常见算法如神经网络算法、随机森林算法以及决策树等，希望能对数据挖掘爱好者有一定帮助。">
<meta property="og:image" content="http://7xiur2.com1.z0.glb.clouddn.com/0063.png">
<meta property="og:image" content="http://7xiur2.com1.z0.glb.clouddn.com/0064.png">
<meta property="og:image" content="http://7xiur2.com1.z0.glb.clouddn.com/0203.png">
<meta property="og:image" content="http://7xiur2.com1.z0.glb.clouddn.com/0204.png">
<meta property="og:image" content="http://7xiur2.com1.z0.glb.clouddn.com/0205.png">
<meta property="og:image" content="http://7xiur2.com1.z0.glb.clouddn.com/0206.png">
<meta property="og:image" content="http://7xiur2.com1.z0.glb.clouddn.com/0207.png">
<meta property="og:image" content="http://7xiur2.com1.z0.glb.clouddn.com/0208.png">
<meta property="og:image" content="http://7xiur2.com1.z0.glb.clouddn.com/0209.png">
<meta property="og:image" content="http://7xiur2.com1.z0.glb.clouddn.com/0210.png">
<meta property="og:image" content="http://7xiur2.com1.z0.glb.clouddn.com/0211.png">
<meta property="og:image" content="http://7xiur2.com1.z0.glb.clouddn.com/0214.png">
<meta property="og:image" content="http://7xiur2.com1.z0.glb.clouddn.com/0215.png">
<meta property="og:image" content="http://7xiur2.com1.z0.glb.clouddn.com/0216.png">
<meta property="og:image" content="http://7xiur2.com1.z0.glb.clouddn.com/0068.png">
<meta property="og:image" content="http://7xiur2.com1.z0.glb.clouddn.com/0069.png">
<meta property="og:image" content="http://7xiur2.com1.z0.glb.clouddn.com/0070.png">
<meta property="og:image" content="http://7xiur2.com1.z0.glb.clouddn.com/0071.png">
<meta property="og:image" content="http://7xiur2.com1.z0.glb.clouddn.com/0073.png">
<meta property="og:image" content="http://7xiur2.com1.z0.glb.clouddn.com/0193.png">
<meta property="og:image" content="http://7xiur2.com1.z0.glb.clouddn.com/0194.png">
<meta property="og:image" content="http://7xiur2.com1.z0.glb.clouddn.com/0195.png">
<meta property="og:image" content="http://7xiur2.com1.z0.glb.clouddn.com/0196.png">
<meta property="og:image" content="http://7xiur2.com1.z0.glb.clouddn.com/0199.png">
<meta property="og:image" content="http://7xiur2.com1.z0.glb.clouddn.com/0200.png">
<meta property="og:image" content="http://7xiur2.com1.z0.glb.clouddn.com/0201.png">
<meta property="og:image" content="http://7xiur2.com1.z0.glb.clouddn.com/0219.png">
<meta property="og:image" content="http://7xiur2.com1.z0.glb.clouddn.com/0220.png">
<meta property="og:image" content="http://7xiur2.com1.z0.glb.clouddn.com/0221.png">
<meta property="og:updated_time" content="2015-09-02T05:48:20.000Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="分类算法">
<meta name="twitter:description" content="本文总结了数据挖掘以及机器学习中常见算法如神经网络算法、随机森林算法以及决策树等，希望能对数据挖掘爱好者有一定帮助。">

    
    <link rel="alternative" href="https://github.com/search?q=pangjiuzala&type=Users" title="XingLiu&#39;s  Blog" type="application/atom+xml">
    
    
    <link rel="icon" href="/img/favicon.ico">
    
    
    <link rel="apple-touch-icon" href="/img/jacman.jpg">
    <link rel="apple-touch-icon-precomposed" href="/img/jacman.jpg">
    
    <link rel="stylesheet" href="/css/style.css" type="text/css">
</head>

  <body>
    <header>
      
<div>
		
			<div id="imglogo">
				<a href="/"><img src="/img/logo.png" alt="XingLiu&#39;s  Blog" title="XingLiu&#39;s  Blog"/></a>
			</div>
			
			<div id="textlogo">
				<h1 class="site-name"><a href="/" title="XingLiu&#39;s  Blog">XingLiu&#39;s  Blog</a></h1>
				<h2 class="blog-motto"></h2>
			</div>
			<div class="navbar"><a class="navbutton navmobile" href="#" title="菜单">
			</a></div>
			<nav class="animated">
				<ul>
					<ul>
					 
						<li><a href="/">主页</a></li>
					
						<li><a href="/archives">文章列表</a></li>
					
					<li>
 					
					<form class="search" action="/search/index.html" method="get" accept-charset="utf-8">
						<label>Search</label>
						<input type="search" id="search" autocomplete="off" name="q" maxlength="20" placeholder="搜索" />
					</form>
					
					</li>
				</ul>
			</nav>			
</div>
    </header>
    <div id="container">
      <div id="main" class="post" itemscope itemprop="blogPost">
  
	<article itemprop="articleBody"> 
		<header class="article-info clearfix">
  <h1 itemprop="name">
    
      <a href="/2015/07/31/分类算法/" title="分类算法" itemprop="url">分类算法</a>
  </h1>
  <p class="article-author">By
       
		<a href="/about" title="XingLiu" target="_blank" itemprop="author">XingLiu</a>
		
  <p class="article-time">
    <time datetime="2015-07-30T23:00:14.000Z" itemprop="datePublished"> 发表于 2015-07-31</time>
    
  </p>
</header>
	<div class="article-content">
		
		<div id="toc" class="toc-article">
			<strong class="toc-title">文章目录</strong>
		
			<ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#BP神经网络"><span class="toc-number">1.</span> <span class="toc-text">BP神经网络</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#决策树"><span class="toc-number">2.</span> <span class="toc-text">决策树</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#简介"><span class="toc-number">2.1.</span> <span class="toc-text">简介</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#组成"><span class="toc-number">2.2.</span> <span class="toc-text">组成</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#画法"><span class="toc-number">2.3.</span> <span class="toc-text">画法</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#决策树的剪枝"><span class="toc-number">2.4.</span> <span class="toc-text">决策树的剪枝</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#决策树总结"><span class="toc-number">2.5.</span> <span class="toc-text">决策树总结</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#树的投票"><span class="toc-number">2.6.</span> <span class="toc-text">树的投票</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#一个映射的例子"><span class="toc-number">2.7.</span> <span class="toc-text">一个映射的例子</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#PageRank算法"><span class="toc-number">3.</span> <span class="toc-text">PageRank算法</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#K-core"><span class="toc-number">4.</span> <span class="toc-text">K-core</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#AdaBoost算法"><span class="toc-number">5.</span> <span class="toc-text">AdaBoost算法</span></a></li></ol>
		
		</div>
		
		<p>本文总结了数据挖掘以及机器学习中常见算法如神经网络算法、随机森林算法以及决策树等，希望能对数据挖掘爱好者有一定帮助。<br><a id="more"></a></p>
<h1 id="BP神经网络">BP神经网络</h1><p>BP网络能学习和存贮大量的输入-输出模式映射关系，而无需事前揭示描述这种映射关系的数学方程。它的学习规则是使用最速下降法，通过反向传播来不断调整网络的权值和阈值，使网络的误差平方和最小。BP神经网络模型拓扑结构包括输入层（input）、隐层(hidelayer)和输出层(outputlayer)。BP神经网络算法是在BP神经网络现有算法的基础上提出的，是通过任意选定一组权值，将给定的目标输出直接作为线性方程的代数和来建立线性方程组，解得待求权，不存在传统方法的局部极小及收敛速度慢的问题，且更易理解。</p>
<p>其基本思想是：由所给的输入、输出模式对通过作用于神经网络来建立线性方程组，运用高斯消元法解线性方程组来求得未知权值，而未采用传统BP网络的非线性函数误差反馈寻优的思想。</p>
<p>对给定的样本模式对，随机选定一组自由权，作为输出层和隐含层之间固定权值，通过传递函数计算隐层的实际输出，再将输出层与隐层间的权值作为待求量，直接将目标输出作为等式的右边建立方程组来求解。</p>
<h1 id="决策树">决策树</h1><h2 id="简介">简介</h2><p>项目风险，判断其可行性的决策分析方法，是直观运用概率分析的一种图解法。由于这种决策分支画成图形很像一棵树的枝干，故称决策树。在机器学习中，决策树是一个预测模型，他代表的是对象属性与对象值之间的一种映射关系。Entropy=系统的凌乱程度，使用算法ID3, C4.5和C5.0生成树算法使用熵。这一度量是基于信息学理论中熵的概念。</p>
<p>决策树是一种树形结构，其中每个内部节点表示一个属性上的测试，每个分支代表一个测试输出，每个叶节点代表一种类别。<br>分类树（决策树）是一种十分常用的分类方法。他是一种监管学习，所谓监管学习就是给定一堆样本，每个样本都有一组属性和一个类别，这些类别是事先确定的，那么通过学习得到一个分类器，这个分类器能够对新出现的对象给出正确的分类。这样的机器学习就被称之为监督学习。</p>
<h2 id="组成">组成</h2><p>□——决策点，是对几种可能方案的选择，即最后选择的最佳方案。如果决策属于多级决策，则决策树的中间可以有多个决策点，以决策树根部的决策点为最终决策方案。<br>○——状态节点，代表备选方案的经济效果（期望值），通过各状态节点的经济效果的对比，按照一定的决策标准就可以选出最佳方案。由状态节点引出的分支称为概率枝，概率枝的数目表示可能出现的自然状态数目每个分枝上要注明该状态出现的概率。<br>△——结果节点，将每个方案在各种自然状态下取得的损益值标注于结果节点的右端。</p>
<h2 id="画法">画法</h2><p>机器学习中，决策树是一个预测模型；他代表的是对象属性与对象值之间的一种映射关系。树中每个节点表示某个对象，而每个分叉路径则代表的某个可能的属性值，而每个叶结点则对应从根节点到该叶节点所经历的路径所表示的对象的值。决策树仅有单一输出，若欲有复数输出，可以建立独立的决策树以处理不同输出。数据挖掘中决策树是一种经常要用到的技术，可以用于分析数据，同样也可以用来作预测。</p>
<p>从数据产生决策树的机器学习技术叫做决策树学习,通俗说就是决策树。</p>
<p>一个决策树包含三种类型的节点：</p>
<p>决策节点：通常用矩形框来表示</p>
<p>机会节点：通常用圆圈来表示</p>
<p>终结点：通常用三角形来表示</p>
<p>决策树学习也是资料探勘中一个普通的方法。在这里，每个决策树都表述了一种树型结构，它由它的分支来对该类型的对象依靠属性进行分类。每个决策树可以依靠对源数据库的分割进行数据测试。这个过程可以递归式的对树进行修剪。当不能再进行分割或一个单独的类可以被应用于某一分支时，递归过程就完成了。另外，随机森林分类器将许多决策树结合起来以提升分类的正确率。</p>
<p>决策树同时也可以依靠计算条件概率来构造。</p>
<p>决策树如果依靠数学的计算方法可以取得更加理想的效果。数据库已如下所示：<br>　　(x,y)=(x1,x2,x3…,xk,y)<br>相关的变量Y表示我们尝试去理解，分类或者更一般化的结果。其他的变量x1,x2,x3等则是帮助我们达到目的的变量。<br>决策树实际上是将空间用超平面进行划分的一种方法，每次分割的时候，都将当前的空间一分为二，比如说下面的决策树：</p>
<center><br><img src="http://7xiur2.com1.z0.glb.clouddn.com/0063.png" alt=""><br></center><br>就是将空间划分成下面的样子：<br><center><br><img src="http://7xiur2.com1.z0.glb.clouddn.com/0064.png" alt=""><br></center>


<h2 id="决策树的剪枝">决策树的剪枝</h2><p>剪枝是决策树停止分支的方法之一，剪枝有分预先剪枝和后剪枝两种。预先剪枝是在树的生长过程中设定一个指标，当达到该指标时就停止生长，这样做容易产生“视界局限”，就是一旦停止分支，使得节点N成为叶节点，就断绝了其后继节点进行“好”的分支操作的任何可能性。不严格的说这些已停止的分支会误导学习算法，导致产生的树不纯度降差最大的地方过分靠近根节点。后剪枝中树首先要充分生长，直到叶节点都有最小的不纯度值为止，因而可以克服“视界局限”。然后对所有相邻的成对叶节点考虑是否消去它们，如果消去能引起令人满意的不纯度增长，那么执行消去，并令它们的公共父节点成为新的叶节点。这种“合并”叶节点的做法和节点分支的过程恰好相反，经过剪枝后叶节点常常会分布在很宽的层次上，树也变得非平衡。后剪枝技术的优点是克服了“视界局限”效应，而且无需保留部分样本用于交叉验证，所以可以充分利用全部训练集的信息。但后剪枝的计算量代价比预剪枝方法大得多，特别是在大样本集中，不过对于小样本的情况，后剪枝方法还是优于预剪枝方法的。</p>
<h2 id="决策树总结">决策树总结</h2><center><br><img src="http://7xiur2.com1.z0.glb.clouddn.com/0203.png" alt=""><br></center><br><center><br><img src="http://7xiur2.com1.z0.glb.clouddn.com/0204.png" alt=""><br></center><br><center><br><img src="http://7xiur2.com1.z0.glb.clouddn.com/0205.png" alt=""><br></center><br><center><br><img src="http://7xiur2.com1.z0.glb.clouddn.com/0206.png" alt=""><br></center><br><center><br><img src="http://7xiur2.com1.z0.glb.clouddn.com/0207.png" alt=""><br></center><br><center><br><img src="http://7xiur2.com1.z0.glb.clouddn.com/0208.png" alt=""><br></center><br><center><br><img src="http://7xiur2.com1.z0.glb.clouddn.com/0209.png" alt=""><br></center><br><center><br><img src="http://7xiur2.com1.z0.glb.clouddn.com/0210.png" alt=""><br></center><br><center><br><img src="http://7xiur2.com1.z0.glb.clouddn.com/0211.png" alt=""><br></center><br><strong>代码实现</strong><br><br><strong>Decisiontree.py</strong><br><figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line">from sklearn import tree</span><br><span class="line">x = [[<span class="number">1</span>,<span class="number">1</span>],[<span class="number">1</span>,<span class="number">1</span>],[<span class="number">1</span>,<span class="number">0</span>],[<span class="number">0</span>,<span class="number">1</span>],[<span class="number">0</span>,<span class="number">1</span>]]</span><br><span class="line">y= [<span class="number">1</span>,<span class="number">1</span>,<span class="number">0</span>,<span class="number">0</span>,<span class="number">0</span>]</span><br><span class="line">clf=tree.DecisionTreeClassifier()</span><br><span class="line">clf= clf.fit(x,y)</span><br><span class="line">print clf.predict([[<span class="number">1</span>,<span class="number">1</span>]])</span><br></pre></td></tr></table></figure><br><br><strong>随机森林</strong><br><center><br><img src="http://7xiur2.com1.z0.glb.clouddn.com/0214.png" alt=""><br></center><br><center><br><img src="http://7xiur2.com1.z0.glb.clouddn.com/0215.png" alt=""><br></center><br><center><br><img src="http://7xiur2.com1.z0.glb.clouddn.com/0216.png" alt=""><br></center>

<h2 id="树的投票">树的投票</h2><p>当你要做预测的时候，新的观察到的特征随着决策树自上而下走下来，这样一组观察到的特征将会被贴上一个预测值/标签。一旦森林中的每棵树都给出了预测值/签，所有的预测结果将被归总到一起，所有树的模式投票被返回做为最终的预测结果。<br>简单来说，99.9%不相关的树做出的预测结果涵盖所有的情况，这些预测结果将会彼此抵消。少数优秀的树的预测结果将会超脱于芸芸“噪音”，做出一个好的预测。</p>
<h2 id="一个映射的例子">一个映射的例子</h2><p>随机森林在没有精心准备的数据映射的情况下也能学习。以方程f(x) = log(x)为例。制造一些假数据，并且加上一点儿噪音。</p>
<figure class="highlight stylus"><table><tr><td class="code"><pre><span class="line">import numpy as np    x = np<span class="class">.random</span><span class="class">.uniform</span>(<span class="number">1</span>, <span class="number">100</span>, <span class="number">1000</span>)    y = np.<span class="function"><span class="title">log</span><span class="params">(x)</span></span> + np<span class="class">.random</span><span class="class">.normal</span>(<span class="number">0</span>, .<span class="number">3</span>, <span class="number">1000</span>)</span><br></pre></td></tr></table></figure>
<center><br><img src="http://7xiur2.com1.z0.glb.clouddn.com/0068.png" alt=""><br></center>



<p>如果我们建立了一个基本的线性模型通过使用 x 来预测y，我们需要作一条直线，算是平分log (x)函数。而如果我们使用一个随机的森林，它不会更好的逼近 log (x)曲线并能够使得它更像实际函数。 </p>
<center><br><img src="http://7xiur2.com1.z0.glb.clouddn.com/0069.png" alt=""><br></center><br>你也许会说随机森林有点扰乱log(x)函数。不管怎样我都认为这做了一个很好的说明如何随机森林并未绑定于线性约束 。<br><br><strong>变量选择</strong><br>随机森林最好的用例之一是特征选择。尝试很多决策树变种的一个副产品就是你可以检测每棵树中哪个变量最合适/最糟糕。<br>当一棵树使用一个变量，而另一棵不使用这个变量，你就可以从是否包含这个变量来比较价值的减少或增加。优秀的随机森林实现将为你做这些事情，所以你需要做的仅仅是知道去看那个方法或参数。<br>在下述的例子中，我们尝试去指出对于将酒分为红酒或者白酒哪个变量是最重要的。<br><center><br><img src="http://7xiur2.com1.z0.glb.clouddn.com/0070.png" alt=""><br></center><br><center><br><img src="http://7xiur2.com1.z0.glb.clouddn.com/0071.png" alt=""><br></center><br><strong>回归</strong><br>随机森林不像其它算法对分类变量或者分类变量和真实变量混合学习的非常好。具有高基数（可能值的#）的分类变量是很棘手的，所以在你的口袋中放点儿这样的东西将会是非常有用的。<br><br><center><br><img src="http://7xiur2.com1.z0.glb.clouddn.com/0073.png" alt=""><br></center>

<p>随机森林相当容易使用，而且很强大。对于任何建模，都要注意过拟合。如果你有兴趣用R语言开始使用随机森林，那么就签出randomForest包。<br><strong>代码实现</strong><br><strong>RandomForest.py</strong><br><figure class="highlight stylus"><table><tr><td class="code"><pre><span class="line">from sklearn<span class="class">.cross_validation</span> import cross_val_score</span><br><span class="line">from sklearn<span class="class">.datasets</span> import make_blobs</span><br><span class="line">from sklearn<span class="class">.ensemble</span> import RandomForestClassifier</span><br><span class="line"></span><br><span class="line">x,y=<span class="function"><span class="title">make_blobs</span><span class="params">(n_samples=<span class="number">10000</span>,n_features=<span class="number">10</span>,centers=<span class="number">100</span>,random_state=<span class="number">0</span>)</span></span></span><br><span class="line">clf=<span class="function"><span class="title">RandomForestClassifier</span><span class="params">(n_estimators=<span class="number">10</span>,max_depth=None,min_samples_split=<span class="number">1</span>,random_state=<span class="number">0</span>)</span></span></span><br><span class="line">scores = <span class="function"><span class="title">cross_val_score</span><span class="params">(clf,x,y)</span></span></span><br><span class="line">print scores.<span class="function"><span class="title">mean</span><span class="params">()</span></span></span><br></pre></td></tr></table></figure></p>
<h1 id="PageRank算法">PageRank算法</h1><p><center><br><img src="http://7xiur2.com1.z0.glb.clouddn.com/0193.png" alt=""><br></center></p>
<p><center><br><img src="http://7xiur2.com1.z0.glb.clouddn.com/0194.png" alt=""><br></center></p>
<p><center><br><img src="http://7xiur2.com1.z0.glb.clouddn.com/0195.png" alt=""><br></center></p>
<p><center><br><img src="http://7xiur2.com1.z0.glb.clouddn.com/0196.png" alt=""><br></center><br><strong>代码实现</strong><br><strong>PageRank.py</strong></p>
<figure class="highlight stylus"><table><tr><td class="code"><pre><span class="line">import networkx as nx</span><br><span class="line">g = nx.<span class="function"><span class="title">DiGraph</span><span class="params">()</span></span></span><br><span class="line">g.<span class="function"><span class="title">add_edge</span><span class="params">(<span class="number">1</span>,<span class="number">2</span>)</span></span></span><br><span class="line">g.<span class="function"><span class="title">add_edge</span><span class="params">(<span class="number">1</span>,<span class="number">3</span>)</span></span></span><br><span class="line">g.<span class="function"><span class="title">add_edge</span><span class="params">(<span class="number">1</span>,<span class="number">4</span>)</span></span></span><br><span class="line">g.<span class="function"><span class="title">add_edge</span><span class="params">(<span class="number">2</span>,<span class="number">1</span>)</span></span></span><br><span class="line">g.<span class="function"><span class="title">add_edge</span><span class="params">(<span class="number">2</span>,<span class="number">4</span>)</span></span></span><br><span class="line">g.<span class="function"><span class="title">add_edge</span><span class="params">(<span class="number">3</span>,<span class="number">1</span>)</span></span></span><br><span class="line">g.<span class="function"><span class="title">add_edge</span><span class="params">(<span class="number">4</span>,<span class="number">2</span>)</span></span></span><br><span class="line">g.<span class="function"><span class="title">add_edge</span><span class="params">(<span class="number">4</span>,<span class="number">3</span>)</span></span></span><br><span class="line">pr = nx.<span class="function"><span class="title">pagerank</span><span class="params">(g,alpha=<span class="number">0.8</span>)</span></span></span><br><span class="line">print pr</span><br></pre></td></tr></table></figure>
<h1 id="K-core">K-core</h1><p><center><br><img src="http://7xiur2.com1.z0.glb.clouddn.com/0199.png" alt=""><br></center></p>
<p><center><br><img src="http://7xiur2.com1.z0.glb.clouddn.com/0200.png" alt=""><br></center></p>
<p><center><br><img src="http://7xiur2.com1.z0.glb.clouddn.com/0201.png" alt=""><br></center><br><strong>代码实现</strong><br><strong>Kmeans.py</strong><br><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> numpy <span class="keyword">import</span> *</span><br><span class="line"><span class="keyword">import</span> time</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"></span><br><span class="line"><span class="comment"># calculate Euclidean distance</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">euclDistance</span><span class="params">(vector1, vector2)</span>:</span></span><br><span class="line">	<span class="keyword">return</span> sqrt(sum(power(vector2 - vector1, <span class="number">2</span>)))</span><br><span class="line"></span><br><span class="line"><span class="comment"># init centroids with random samples</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">initCentroids</span><span class="params">(dataSet, k)</span>:</span></span><br><span class="line">	numSamples, dim = dataSet.shape</span><br><span class="line">	centroids = zeros((k, dim))</span><br><span class="line">	<span class="keyword">for</span> i <span class="keyword">in</span> range(k):</span><br><span class="line">		index = int(random.uniform(<span class="number">0</span>, numSamples))</span><br><span class="line">		centroids[i, :] = dataSet[index, :]</span><br><span class="line">	<span class="keyword">return</span> centroids</span><br><span class="line"></span><br><span class="line"><span class="comment"># k-means cluster</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">kmeans</span><span class="params">(dataSet, k)</span>:</span></span><br><span class="line">	numSamples = dataSet.shape[<span class="number">0</span>]</span><br><span class="line">	<span class="comment"># first column stores which cluster this sample belongs to,</span></span><br><span class="line">	<span class="comment"># second column stores the error between this sample and its centroid</span></span><br><span class="line">	clusterAssment = mat(zeros((numSamples, <span class="number">2</span>)))</span><br><span class="line">	clusterChanged = <span class="keyword">True</span></span><br><span class="line"></span><br><span class="line">	<span class="comment">## step 1: init centroids</span></span><br><span class="line">	centroids = initCentroids(dataSet, k)</span><br><span class="line"></span><br><span class="line">	<span class="keyword">while</span> clusterChanged:</span><br><span class="line">		clusterChanged = <span class="keyword">False</span></span><br><span class="line">		<span class="comment">## for each sample</span></span><br><span class="line">		<span class="keyword">for</span> i <span class="keyword">in</span> xrange(numSamples):</span><br><span class="line">			minDist  = <span class="number">100000.0</span></span><br><span class="line">			minIndex = <span class="number">0</span></span><br><span class="line">			<span class="comment">## for each centroid</span></span><br><span class="line">			<span class="comment">## step 2: find the centroid who is closest</span></span><br><span class="line">			<span class="keyword">for</span> j <span class="keyword">in</span> range(k):</span><br><span class="line">				distance = euclDistance(centroids[j, :], dataSet[i, :])</span><br><span class="line">				<span class="keyword">if</span> distance &lt; minDist:</span><br><span class="line">					minDist  = distance</span><br><span class="line">					minIndex = j</span><br><span class="line">			</span><br><span class="line">			<span class="comment">## step 3: update its cluster</span></span><br><span class="line">			<span class="keyword">if</span> clusterAssment[i, <span class="number">0</span>] != minIndex:</span><br><span class="line">				clusterChanged = <span class="keyword">True</span></span><br><span class="line">				clusterAssment[i, :] = minIndex, minDist**<span class="number">2</span></span><br><span class="line"></span><br><span class="line">		<span class="comment">## step 4: update centroids</span></span><br><span class="line">		<span class="keyword">for</span> j <span class="keyword">in</span> range(k):</span><br><span class="line">			pointsInCluster = dataSet[nonzero(clusterAssment[:, <span class="number">0</span>].A == j)[<span class="number">0</span>]]</span><br><span class="line">			centroids[j, :] = mean(pointsInCluster, axis = <span class="number">0</span>)</span><br><span class="line"></span><br><span class="line">	<span class="keyword">print</span> <span class="string">'Congratulations, cluster complete!'</span></span><br><span class="line">	<span class="keyword">return</span> centroids, clusterAssment</span><br><span class="line"></span><br><span class="line"><span class="comment"># show your cluster only available with 2-D data</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">showCluster</span><span class="params">(dataSet, k, centroids, clusterAssment)</span>:</span></span><br><span class="line">	numSamples, dim = dataSet.shape</span><br><span class="line">	<span class="keyword">if</span> dim != <span class="number">2</span>:</span><br><span class="line">		<span class="keyword">print</span> <span class="string">"Sorry! I can not draw because the dimension of your data is not 2!"</span></span><br><span class="line">		<span class="keyword">return</span> <span class="number">1</span></span><br><span class="line"></span><br><span class="line">	mark = [<span class="string">'or'</span>, <span class="string">'ob'</span>, <span class="string">'og'</span>, <span class="string">'ok'</span>, <span class="string">'^r'</span>, <span class="string">'+r'</span>, <span class="string">'sr'</span>, <span class="string">'dr'</span>, <span class="string">'&lt;r'</span>, <span class="string">'pr'</span>]</span><br><span class="line">	<span class="keyword">if</span> k &gt; len(mark):</span><br><span class="line">		<span class="keyword">print</span> <span class="string">"Sorry! Your k is too large! please contact Zouxy"</span></span><br><span class="line">		<span class="keyword">return</span> <span class="number">1</span></span><br><span class="line"></span><br><span class="line">	<span class="comment"># draw all samples</span></span><br><span class="line">	<span class="keyword">for</span> i <span class="keyword">in</span> xrange(numSamples):</span><br><span class="line">		markIndex = int(clusterAssment[i, <span class="number">0</span>])</span><br><span class="line">		plt.plot(dataSet[i, <span class="number">0</span>], dataSet[i, <span class="number">1</span>], mark[markIndex])</span><br><span class="line"></span><br><span class="line">	mark = [<span class="string">'Dr'</span>, <span class="string">'Db'</span>, <span class="string">'Dg'</span>, <span class="string">'Dk'</span>, <span class="string">'^b'</span>, <span class="string">'+b'</span>, <span class="string">'sb'</span>, <span class="string">'db'</span>, <span class="string">'&lt;b'</span>, <span class="string">'pb'</span>]</span><br><span class="line">	<span class="comment"># draw the centroids</span></span><br><span class="line">	<span class="keyword">for</span> i <span class="keyword">in</span> range(k):</span><br><span class="line">		plt.plot(centroids[i, <span class="number">0</span>], centroids[i, <span class="number">1</span>], mark[i], markersize = <span class="number">12</span>)</span><br><span class="line"></span><br><span class="line">	plt.show()</span><br><span class="line"><span class="keyword">print</span> <span class="string">"step 1: load data..."</span></span><br><span class="line">dataSet = []</span><br><span class="line">fileIn = open(<span class="string">'C:/Users/Administrator/test/testSet.txt'</span>)</span><br><span class="line"><span class="keyword">for</span> line <span class="keyword">in</span> fileIn.readlines():</span><br><span class="line">	lineArr = line.strip().split(<span class="string">'\t'</span>)</span><br><span class="line">	dataSet.append([float(lineArr[<span class="number">0</span>]), float(lineArr[<span class="number">1</span>])])</span><br><span class="line"></span><br><span class="line"><span class="comment">## step 2: clustering...</span></span><br><span class="line"><span class="keyword">print</span> <span class="string">"step 2: clustering..."</span></span><br><span class="line">dataSet = mat(dataSet)</span><br><span class="line">k = <span class="number">4</span></span><br><span class="line">centroids, clusterAssment = kmeans(dataSet, k)</span><br><span class="line"></span><br><span class="line"><span class="comment">## step 3: show the result</span></span><br><span class="line"><span class="keyword">print</span> <span class="string">"step 3: show the result..."</span></span><br><span class="line">showCluster(dataSet, k, centroids, clusterAssment)</span><br></pre></td></tr></table></figure></p>
<p><strong>testSet.txt</strong><br><figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="number">1.658985</span>	<span class="number">4.285136</span></span><br><span class="line">-<span class="number">3.453687</span>	<span class="number">3.424321</span></span><br><span class="line"><span class="number">4.838138</span>	-<span class="number">1.151539</span></span><br><span class="line">-<span class="number">5.379713</span>	-<span class="number">3.362104</span></span><br><span class="line"><span class="number">0.972564</span>	<span class="number">2.924086</span></span><br><span class="line">-<span class="number">3.567919</span>	<span class="number">1.531611</span></span><br><span class="line"><span class="number">0.450614</span>	-<span class="number">3.302219</span></span><br><span class="line">-<span class="number">3.487105</span>	-<span class="number">1.724432</span></span><br><span class="line"><span class="number">2.668759</span>	<span class="number">1.594842</span></span><br><span class="line">-<span class="number">3.156485</span>	<span class="number">3.191137</span></span><br><span class="line"><span class="number">3.165506</span>	-<span class="number">3.999838</span></span><br><span class="line">-<span class="number">2.786837</span>	-<span class="number">3.099354</span></span><br><span class="line"><span class="number">4.208187</span>	<span class="number">2.984927</span></span><br><span class="line">-<span class="number">2.123337</span>	<span class="number">2.943366</span></span><br><span class="line"><span class="number">0.704199</span>	-<span class="number">0.479481</span></span><br><span class="line">-<span class="number">0.392370</span>	-<span class="number">3.963704</span></span><br><span class="line"><span class="number">2.831667</span>	<span class="number">1.574018</span></span><br><span class="line">-<span class="number">0.790153</span>	<span class="number">3.343144</span></span><br><span class="line"><span class="number">2.943496</span>	-<span class="number">3.357075</span></span><br><span class="line">-<span class="number">3.195883</span>	-<span class="number">2.283926</span></span><br><span class="line"><span class="number">2.336445</span>	<span class="number">2.875106</span></span><br><span class="line">-<span class="number">1.786345</span>	<span class="number">2.554248</span></span><br><span class="line"><span class="number">2.190101</span>	-<span class="number">1.906020</span></span><br><span class="line">-<span class="number">3.403367</span>	-<span class="number">2.778288</span></span><br><span class="line"><span class="number">1.778124</span>	<span class="number">3.880832</span></span><br><span class="line">-<span class="number">1.688346</span>	<span class="number">2.230267</span></span><br><span class="line"><span class="number">2.592976</span>	-<span class="number">2.054368</span></span><br><span class="line">-<span class="number">4.007257</span>	-<span class="number">3.207066</span></span><br><span class="line"><span class="number">2.257734</span>	<span class="number">3.387564</span></span><br><span class="line">-<span class="number">2.679011</span>	<span class="number">0.785119</span></span><br><span class="line"><span class="number">0.939512</span>	-<span class="number">4.023563</span></span><br><span class="line">-<span class="number">3.674424</span>	-<span class="number">2.261084</span></span><br><span class="line"><span class="number">2.046259</span>	<span class="number">2.735279</span></span><br><span class="line">-<span class="number">3.189470</span>	<span class="number">1.780269</span></span><br><span class="line"><span class="number">4.372646</span>	-<span class="number">0.822248</span></span><br><span class="line">-<span class="number">2.579316</span>	-<span class="number">3.497576</span></span><br><span class="line"><span class="number">1.889034</span>	<span class="number">5.190400</span></span><br><span class="line">-<span class="number">0.798747</span>	<span class="number">2.185588</span></span><br><span class="line"><span class="number">2.836520</span>	-<span class="number">2.658556</span></span><br><span class="line">-<span class="number">3.837877</span>	-<span class="number">3.253815</span></span><br><span class="line"><span class="number">2.096701</span>	<span class="number">3.886007</span></span><br><span class="line">-<span class="number">2.709034</span>	<span class="number">2.923887</span></span><br><span class="line"><span class="number">3.367037</span>	-<span class="number">3.184789</span></span><br><span class="line">-<span class="number">2.121479</span>	-<span class="number">4.232586</span></span><br><span class="line"><span class="number">2.329546</span>	<span class="number">3.179764</span></span><br><span class="line">-<span class="number">3.284816</span>	<span class="number">3.273099</span></span><br><span class="line"><span class="number">3.091414</span>	-<span class="number">3.815232</span></span><br><span class="line">-<span class="number">3.762093</span>	-<span class="number">2.432191</span></span><br><span class="line"><span class="number">3.542056</span>	<span class="number">2.778832</span></span><br><span class="line">-<span class="number">1.736822</span>	<span class="number">4.241041</span></span><br><span class="line"><span class="number">2.127073</span>	-<span class="number">2.983680</span></span><br><span class="line">-<span class="number">4.323818</span>	-<span class="number">3.938116</span></span><br><span class="line"><span class="number">3.792121</span>	<span class="number">5.135768</span></span><br><span class="line">-<span class="number">4.786473</span>	<span class="number">3.358547</span></span><br><span class="line"><span class="number">2.624081</span>	-<span class="number">3.260715</span></span><br><span class="line">-<span class="number">4.009299</span>	-<span class="number">2.978115</span></span><br><span class="line"><span class="number">2.493525</span>	<span class="number">1.963710</span></span><br><span class="line">-<span class="number">2.513661</span>	<span class="number">2.642162</span></span><br><span class="line"><span class="number">1.864375</span>	-<span class="number">3.176309</span></span><br><span class="line">-<span class="number">3.171184</span>	-<span class="number">3.572452</span></span><br><span class="line"><span class="number">2.894220</span>	<span class="number">2.489128</span></span><br><span class="line">-<span class="number">2.562539</span>	<span class="number">2.884438</span></span><br><span class="line"><span class="number">3.491078</span>	-<span class="number">3.947487</span></span><br><span class="line">-<span class="number">2.565729</span>	-<span class="number">2.012114</span></span><br><span class="line"><span class="number">3.332948</span>	<span class="number">3.983102</span></span><br><span class="line">-<span class="number">1.616805</span>	<span class="number">3.573188</span></span><br><span class="line"><span class="number">2.280615</span>	-<span class="number">2.559444</span></span><br><span class="line">-<span class="number">2.651229</span>	-<span class="number">3.103198</span></span><br><span class="line"><span class="number">2.321395</span>	<span class="number">3.154987</span></span><br><span class="line">-<span class="number">1.685703</span>	<span class="number">2.939697</span></span><br><span class="line"><span class="number">3.031012</span>	-<span class="number">3.620252</span></span><br><span class="line">-<span class="number">4.599622</span>	-<span class="number">2.185829</span></span><br><span class="line"><span class="number">4.196223</span>	<span class="number">1.126677</span></span><br><span class="line">-<span class="number">2.133863</span>	<span class="number">3.093686</span></span><br><span class="line"><span class="number">4.668892</span>	-<span class="number">2.562705</span></span><br><span class="line">-<span class="number">2.793241</span>	-<span class="number">2.149706</span></span><br><span class="line"><span class="number">2.884105</span>	<span class="number">3.043438</span></span><br><span class="line">-<span class="number">2.967647</span>	<span class="number">2.848696</span></span><br><span class="line"><span class="number">4.479332</span>	-<span class="number">1.764772</span></span><br><span class="line">-<span class="number">4.905566</span>	-<span class="number">2.911070</span></span><br></pre></td></tr></table></figure></p>
<h1 id="AdaBoost算法">AdaBoost算法</h1><p><center><br><img src="http://7xiur2.com1.z0.glb.clouddn.com/0219.png" alt=""><br></center></p>
<p><center><br><img src="http://7xiur2.com1.z0.glb.clouddn.com/0220.png" alt=""><br></center></p>
<p><center><br><img src="http://7xiur2.com1.z0.glb.clouddn.com/0221.png" alt=""><br></center><br><strong>代码实现</strong><br><strong>AdaBoost.py</strong><br><figure class="highlight stylus"><table><tr><td class="code"><pre><span class="line">from sklearn<span class="class">.cross_validation</span> import cross_val_score</span><br><span class="line">from sklearn<span class="class">.datasets</span> import load_iris</span><br><span class="line">from sklearn<span class="class">.ensemble</span> import AdaBoostClassifier</span><br><span class="line">iris = <span class="function"><span class="title">load_iris</span><span class="params">()</span></span></span><br><span class="line">clf= <span class="function"><span class="title">AdaBoostClassifier</span><span class="params">(n_estimators=<span class="number">100</span>)</span></span></span><br><span class="line">scores = <span class="function"><span class="title">cross_val_score</span><span class="params">(clf,iris.data,iris.target)</span></span></span><br><span class="line">print scores.<span class="function"><span class="title">mean</span><span class="params">()</span></span></span><br></pre></td></tr></table></figure></p>
  
	</div>
		<footer class="article-footer clearfix">
<div class="article-catetags">


  <div class="article-tags">
  
  <span></span> <a href="/tags/数据挖掘/">数据挖掘</a><a href="/tags/算法/">算法</a>
  </div>

</div>



	<div class="article-share" id="share">
	
	<div class="share-jiathis">
	  
<div class="jiathis_style_24x24">
	<a class="jiathis_button_tsina"></a>
	<a class="jiathis_button_weixin"></a>
	<a class="jiathis_button_renren"></a>
	<a class="jiathis_button_qzone"></a>
	<a class="jiathis_button_googleplus"></a>
	<a class="jiathis_button_douban"></a>
	<a href="http://www.jiathis.com/share" class="jiathis jiathis_txt jtico jtico_jiathis" target="_blank"></a>
	<a class="jiathis_counter_style"></a>
</div>
<script type="text/javascript" >
    var jiathis_config={
    data_track_clickback:true,
    sm:"copy,renren,cqq",
    pic:"",
    summary:"",
    
  </script> 
<script type="text/javascript" src="//v3.jiathis.com/code/jia.js?uid=
" charset="utf-8"></script>      

	 </div>
	
	</div>


</footer>

   	       
	</article>
	
<nav class="article-nav clearfix">
 
 <div class="prev" >
 <a href="/2015/08/01/一种面向高维数据的集成聚类算法/" title="一种面向高维数据的集成聚类算法">
  <strong>上一篇：</strong><br/>
  <span>
  一种面向高维数据的集成聚类算法</span>
</a>
</div>


<div class="next">
<a href="/2015/07/30/数据挖掘笔记/"  title="数据挖掘笔记">
 <strong>下一篇：</strong><br/> 
 <span>数据挖掘笔记
</span>
</a>
</div>

</nav>

	
<section id="comments" class="comment">
	<div class="ds-thread" data-thread-key="2015/07/31/分类算法/" data-title="分类算法" data-url="http://pangjiuzala.github.io/2015/07/31/分类算法/"></div>
</section>


</div>  
      <div class="openaside"><a class="navbutton" href="#" title="显示侧边栏"></a></div>

  <div id="toc" class="toc-aside">
  <strong class="toc-title">文章目录</strong>
 
 <ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#BP神经网络"><span class="toc-number">1.</span> <span class="toc-text">BP神经网络</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#决策树"><span class="toc-number">2.</span> <span class="toc-text">决策树</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#简介"><span class="toc-number">2.1.</span> <span class="toc-text">简介</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#组成"><span class="toc-number">2.2.</span> <span class="toc-text">组成</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#画法"><span class="toc-number">2.3.</span> <span class="toc-text">画法</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#决策树的剪枝"><span class="toc-number">2.4.</span> <span class="toc-text">决策树的剪枝</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#决策树总结"><span class="toc-number">2.5.</span> <span class="toc-text">决策树总结</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#树的投票"><span class="toc-number">2.6.</span> <span class="toc-text">树的投票</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#一个映射的例子"><span class="toc-number">2.7.</span> <span class="toc-text">一个映射的例子</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#PageRank算法"><span class="toc-number">3.</span> <span class="toc-text">PageRank算法</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#K-core"><span class="toc-number">4.</span> <span class="toc-text">K-core</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#AdaBoost算法"><span class="toc-number">5.</span> <span class="toc-text">AdaBoost算法</span></a></li></ol>
 
  </div>

<div id="asidepart">
<div class="closeaside"><a class="closebutton" href="#" title="隐藏侧边栏"></a></div>
<aside class="clearfix">

  <div class="weiboshow">
  <p class="asidetitle">新浪微博</p>
    <iframe width="100%" height="119" class="share_self"  frameborder="0" scrolling="no" src="http://widget.weibo.com/weiboshow/index.php?language=&width=0&height=119&fansRow=2&ptype=1&speed=0&skin=9&isTitle=1&noborder=1&isWeibo=0&isFans=0&uid=2129798793&verifier=c0951e84&dpc=1"></iframe>
</div>


  
<div class="tagslist">
	<p class="asidetitle">标签</p>
		<ul class="clearfix">
		
			
				<li><a href="/tags/Java/" title="Java">Java<sup>31</sup></a></li>
			
		
			
				<li><a href="/tags/大数据/" title="大数据">大数据<sup>26</sup></a></li>
			
		
			
				<li><a href="/tags/性能优化/" title="性能优化">性能优化<sup>14</sup></a></li>
			
		
			
				<li><a href="/tags/数据挖掘/" title="数据挖掘">数据挖掘<sup>13</sup></a></li>
			
		
			
				<li><a href="/tags/HBase/" title="HBase">HBase<sup>12</sup></a></li>
			
		
			
				<li><a href="/tags/Hadoop/" title="Hadoop">Hadoop<sup>10</sup></a></li>
			
		
			
				<li><a href="/tags/算法/" title="算法">算法<sup>10</sup></a></li>
			
		
			
				<li><a href="/tags/Spark/" title="Spark">Spark<sup>6</sup></a></li>
			
		
			
				<li><a href="/tags/机器学习/" title="机器学习">机器学习<sup>6</sup></a></li>
			
		
			
				<li><a href="/tags/数据库/" title="数据库">数据库<sup>5</sup></a></li>
			
		
			
				<li><a href="/tags/网络爬虫/" title="网络爬虫">网络爬虫<sup>3</sup></a></li>
			
		
			
				<li><a href="/tags/移动互联网/" title="移动互联网">移动互联网<sup>3</sup></a></li>
			
		
			
				<li><a href="/tags/物联网/" title="物联网">物联网<sup>2</sup></a></li>
			
		
			
				<li><a href="/tags/Android/" title="Android">Android<sup>2</sup></a></li>
			
		
			
				<li><a href="/tags/云平台/" title="云平台">云平台<sup>2</sup></a></li>
			
		
			
				<li><a href="/tags/Taobao-File-System/" title="Taobao File System">Taobao File System<sup>1</sup></a></li>
			
		
			
				<li><a href="/tags/智慧医疗/" title="智慧医疗">智慧医疗<sup>1</sup></a></li>
			
		
			
				<li><a href="/tags/设计模式/" title="设计模式">设计模式<sup>1</sup></a></li>
			
		
			
				<li><a href="/tags/爬虫/" title="爬虫">爬虫<sup>1</sup></a></li>
			
		
			
				<li><a href="/tags/云计算/" title="云计算">云计算<sup>1</sup></a></li>
			
		
		</ul>
</div>


  
  <div class="archiveslist">
    <p class="asidetitle"><a href="/archives">归档</a></p>
      <ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2016/02/">February 2016</a><span class="archive-list-count">4</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2016/01/">January 2016</a><span class="archive-list-count">2</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2015/12/">December 2015</a><span class="archive-list-count">8</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2015/11/">November 2015</a><span class="archive-list-count">5</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2015/10/">October 2015</a><span class="archive-list-count">4</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2015/09/">September 2015</a><span class="archive-list-count">17</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2015/08/">August 2015</a><span class="archive-list-count">31</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2015/07/">July 2015</a><span class="archive-list-count">25</span></li></ul>
  </div>


  <div class="linkslist">
  <p class="asidetitle">友情链接</p>
    <ul>
        
          <li>
            
            	<a href="http://blog.csdn.net/pangjiuzala" target="_blank" title="My Blog in  CSDN">CSDN</a>
            
          </li>
        
          <li>
            
            	<a href="http://saliormoon.github.io/" target="_blank" title="王宇峰的博客">王宇峰的博客</a>
            
          </li>
        
          <li>
            
            	<a href="http://www.changhuiyuan.com/" target="_blank" title="常惠源的博客">常惠源的博客</a>
            
          </li>
        
    </ul>
</div>

  <div class="rsspart">
	<a href="https://github.com/search?q=pangjiuzala&amp;type=Users" target="_blank" title="关注刘兴的github">关注</a>
</div>

  

  


</aside>
</div>
    </div>
    <footer><div id="footer" >
	
	<div class="line">
		<span></span>
		<div class="author"></div>
	</div>
	
	
	<section class="info">
		<p> Hello,I&#39;m from ZjuCs! <br/>
			The more you diligent, the more you lucky!</p>
	</section>
	 
	<div class="social-font" class="clearfix">
		
		
		
		
		
		
		
		
		
		
	</div>
			
		

		<p class="copyright">
		Copyright@ 2015 Liuxing All rights reserved.
		</p>
</div>
</footer>
    <script src="/js/jquery-2.0.3.min.js"></script>
<script src="/js/jquery.imagesloaded.min.js"></script>
<script src="/js/gallery.js"></script>
<script src="/js/jquery.qrcode-0.12.0.min.js"></script>

<script type="text/javascript">
$(document).ready(function(){ 
  $('.navbar').click(function(){
    $('header nav').toggleClass('shownav');
  });
  var myWidth = 0;
  function getSize(){
    if( typeof( window.innerWidth ) == 'number' ) {
      myWidth = window.innerWidth;
    } else if( document.documentElement && document.documentElement.clientWidth) {
      myWidth = document.documentElement.clientWidth;
    };
  };
  var m = $('#main'),
      a = $('#asidepart'),
      c = $('.closeaside'),
      o = $('.openaside');
  c.click(function(){
    a.addClass('fadeOut').css('display', 'none');
    o.css('display', 'block').addClass('fadeIn');
    m.addClass('moveMain');
  });
  o.click(function(){
    o.css('display', 'none').removeClass('beforeFadeIn');
    a.css('display', 'block').removeClass('fadeOut').addClass('fadeIn');      
    m.removeClass('moveMain');
  });
  $(window).scroll(function(){
    o.css("top",Math.max(80,260-$(this).scrollTop()));
  });
  
  $(window).resize(function(){
    getSize(); 
    if (myWidth >= 1024) {
      $('header nav').removeClass('shownav');
    }else{
      m.removeClass('moveMain');
      a.css('display', 'block').removeClass('fadeOut');
      o.css('display', 'none');
      
      $('#toc.toc-aside').css('display', 'none');
        
    }
  });
});
</script>

<script type="text/javascript">
$(document).ready(function(){ 
  var ai = $('.article-content>iframe'),
      ae = $('.article-content>embed'),
      t  = $('#toc'),
      ta = $('#toc.toc-aside'),
      o  = $('.openaside'),
      c  = $('.closeaside');
  if(ai.length>0){
    ai.wrap('<div class="video-container" />');
  };
  if(ae.length>0){
   ae.wrap('<div class="video-container" />');
  };
  c.click(function(){
    ta.css('display', 'block').addClass('fadeIn');
  });
  o.click(function(){
    ta.css('display', 'none');
  });
  $(window).scroll(function(){
    ta.css("top",Math.max(140,320-$(this).scrollTop()));
  });
});
</script>




<script type="text/javascript">
  var duoshuoQuery = {short_name:"pangjiuzala"};
  (function() {
    var ds = document.createElement('script');
    ds.type = 'text/javascript';ds.async = true;
    ds.src = '//static.duoshuo.com/embed.js';
    ds.charset = 'UTF-8';
    (document.getElementsByTagName('head')[0] 
    || document.getElementsByTagName('body')[0]).appendChild(ds);
  })();
</script> 







<link rel="stylesheet" href="/fancybox/jquery.fancybox.css" media="screen" type="text/css">
<script src="/fancybox/jquery.fancybox.pack.js"></script>
<script type="text/javascript">
$(document).ready(function(){ 
  $('.article-content').each(function(i){
    $(this).find('img').each(function(){
      if ($(this).parent().hasClass('fancybox')) return;
      var alt = this.alt;
      if (alt) $(this).after('<span class="caption">' + alt + '</span>');
      $(this).wrap('<a href="' + this.src + '" title="' + alt + '" class="fancybox"></a>');
    });
    $(this).find('.fancybox').each(function(){
      $(this).attr('rel', 'article' + i);
    });
  });
  if($.fancybox){
    $('.fancybox').fancybox();
  }
}); 
</script>



<!-- Analytics Begin -->

<script type="text/javascript">
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');
ga('create', 'null', 'null');  
ga('send', 'pageview');
</script>



<script type="text/javascript">
var _bdhmProtocol = (("https:" == document.location.protocol) ? " https://" : " http://");
document.write(unescape("%3Cscript src='" + _bdhmProtocol + "hm.baidu.com/h.js%3Ffeafc504b70a541dd3845d467335f367' type='text/javascript'%3E%3C/script%3E"));
</script>



<script type="text/javascript">var cnzz_protocol = (("https:" == document.location.protocol) ? " https://" : " http://");document.write(unescape("%3Cspan id='cnzz_stat_icon_null'%3E%3C/span%3E%3Cscript src='" + cnzz_protocol + "s23.cnzz.com/z_stat.php%3Fid%3Dnull' type='text/javascript'%3E%3C/script%3E"));</script>

<!-- Analytics End -->

<!-- Totop Begin -->

	<div id="totop">
	<a title="返回顶部"><img src="/img/scrollup.png"/></a>
	</div>
	<script src="/js/totop.js"></script>

<!-- Totop End -->

<!-- MathJax Begin -->
<!-- mathjax config similar to math.stackexchange -->

<script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    tex2jax: {
      inlineMath: [ ['$','$'], ["\\(","\\)"] ],
      processEscapes: true
    }
  });
</script>

<script type="text/x-mathjax-config">
    MathJax.Hub.Config({
      tex2jax: {
        skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
      }
    });
</script>

<script type="text/x-mathjax-config">
    MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax(), i;
        for(i=0; i < all.length; i += 1) {
            all[i].SourceElement().parentNode.className += ' has-jax';
        }
    });
</script>

<script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script>


<!-- MathJax End -->

<!-- Tiny_search Begin -->

<script>
var option = {
  engineKey: '4ac092ad8d749fdc6293'
};
(function(w,d,t,u,n,s,e){
  s = d.createElement(t);
  s.src = u;
  s.async = 1;
  w[n] = function(r){
    w[n].opts = r;
  };
  e = d.getElementsByTagName(t)[0];
  e.parentNode.insertBefore(s, e);
})(window,document,'script','//tinysou-cdn.b0.upaiyun.com/ts.js','_ts');
_ts(option);
</script>

<!-- Tiny_search End -->

  </body>
</html>
